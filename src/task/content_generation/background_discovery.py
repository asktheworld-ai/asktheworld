"""
Background Discovery component for AI-driven content generation.

This component conducts comprehensive background research on a given topic,
providing the foundational information needed for content creation.
"""

import asyncio
from typing import Optional

from pydantic import Field

from src import log
from src.core import DataCore
from src.lib.model.txt.lm_basis import LmBasis
from src.lib.model.txt.lm_gentxt_params import LmGentxtParams
from src.lib.model.txt.lm_gentxt_result import LmGentxtResult

from .prompts import (
    BACKGROUND_DISCOVERY_PROMPT_TEMPLATE,
    BACKGROUND_DISCOVERY_SYSTEM_PROMPT,
)


class BackgroundDiscoveryParams(DataCore):
    """Parameters for background discovery."""

    topic: str = Field(
        description="The main topic for background research and discovery"
    )

    max_tokens: Optional[int] = Field(
        default=4000, description="Maximum tokens for the LLM response"
    )

    temperature: float = Field(
        default=0.3,
        description="Temperature for LLM generation (lower for more factual content)",
    )


class BackgroundDiscoveryResult(DataCore):
    """Result from background discovery."""

    topic: str = Field(description="The original topic that was researched")

    background_research: str = Field(
        description="Comprehensive background research and analysis"
    )

    input_tokens: int = Field(description="Number of input tokens used by the LLM")

    output_tokens: int = Field(
        description="Number of output tokens generated by the LLM"
    )


class BackgroundDiscovery:
    """
    Background Discovery Component.

    Conducts comprehensive research and background discovery for a given topic,
    providing the foundation information needed for content creation.
    """

    def __init__(self, model: LmBasis, log_name: str = "background_discovery"):
        """
        Initialize the background discovery component.

        Args:
            model: Language model to use for background discovery
            log_name: Name for logging purposes
        """
        self.model = model
        self.logger = log.bind(component=log_name)

    def discover(self, params: BackgroundDiscoveryParams) -> BackgroundDiscoveryResult:
        """
        Conduct synchronous background discovery for a topic.

        Args:
            params: Parameters for background discovery

        Returns:
            BackgroundDiscoveryResult containing comprehensive research
        """
        self.logger.info(f"Starting background discovery for topic: {params.topic}")

        # Prepare the prompt
        prompt = BACKGROUND_DISCOVERY_PROMPT_TEMPLATE.format(topic=params.topic)

        # Create LLM parameters
        llm_params = LmGentxtParams(
            system_prompt=BACKGROUND_DISCOVERY_SYSTEM_PROMPT,
            prompt=prompt,
            max_new_tokens=params.max_tokens,
            temperature=params.temperature,
        )

        # Call the LLM
        llm_result: LmGentxtResult = self.model.gentxt(llm_params)

        # Create and return result
        result = BackgroundDiscoveryResult(
            topic=params.topic,
            background_research=llm_result.output,
            input_tokens=llm_result.input_tokens,
            output_tokens=llm_result.output_tokens,
        )

        self.logger.success(f"Background discovery completed for: {params.topic}")
        self.logger.info(f"Generated {result.output_tokens} tokens of research")

        return result

    async def adiscover(
        self, params: BackgroundDiscoveryParams
    ) -> BackgroundDiscoveryResult:
        """
        Conduct asynchronous background discovery for a topic.

        Args:
            params: Parameters for background discovery

        Returns:
            BackgroundDiscoveryResult containing comprehensive research
        """
        self.logger.info(
            f"Starting async background discovery for topic: {params.topic}"
        )

        # Prepare the prompt
        prompt = BACKGROUND_DISCOVERY_PROMPT_TEMPLATE.format(topic=params.topic)

        # Create LLM parameters
        llm_params = LmGentxtParams(
            system_prompt=BACKGROUND_DISCOVERY_SYSTEM_PROMPT,
            prompt=prompt,
            max_new_tokens=params.max_tokens,
            temperature=params.temperature,
        )

        # Call the LLM asynchronously
        llm_result: LmGentxtResult = await self.model.agentxt(llm_params)

        # Create and return result
        result = BackgroundDiscoveryResult(
            topic=params.topic,
            background_research=llm_result.output,
            input_tokens=llm_result.input_tokens,
            output_tokens=llm_result.output_tokens,
        )

        self.logger.success(f"Async background discovery completed for: {params.topic}")
        self.logger.info(f"Generated {result.output_tokens} tokens of research")

        return result
